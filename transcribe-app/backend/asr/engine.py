from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

from .langid import LanguageIdentifier
from .postprocess.dialect_map import DialectMapper
from .postprocess.itn_th import inverse_text_normalize
from .postprocess.normalize_th import normalize_text
from .postprocess.punct_restore import restore_punctuation
from .vad import VADSegmenter
from .diarization import Diarizer
from .types import Segment


@dataclass
class TranscriptionOptions:
    model_size: str = "small"
    language_hint: Optional[str] = None
    enable_diarization: bool = True
    enable_punct: bool = True
    enable_itn: bool = True
    enable_dialect_map: bool = False
    custom_lexicon: Optional[List[str]] = None
    context_prompt: Optional[str] = None


@dataclass
class TranscriptionResult:
    text: str
    segments: List[Segment]
    metadata: Dict[str, str]
    dialect_mapped_text: Optional[str] = None


class DummyWhisperBackend:
    """A lightweight backend used for tests and environments without models."""

    def __init__(self) -> None:
        self.name = "dummy"

    def transcribe(
        self,
        audio_path: Path,
        options: TranscriptionOptions,
        prompt_bias: Optional[str] = None,
    ) -> List[Segment]:
        # In test environments we allow storing ground-truth transcripts as JSON
        # next to the audio file. This keeps the pipeline deterministic while the
        # production deployment can swap this backend with a FasterWhisper-backed
        # implementation.
        transcript_json = audio_path.with_suffix(".json")
        if transcript_json.exists():
            payload = json.loads(transcript_json.read_text(encoding="utf-8"))
            return [
                Segment(
                    start=seg.get("start", 0.0),
                    end=seg.get("end", 0.0),
                    text=seg.get("text", ""),
                    confidence=seg.get("confidence", 0.9),
                    speaker=seg.get("speaker"),
                    language=seg.get("language"),
                    words=seg.get("words", []),
                )
                for seg in payload.get("segments", [])
            ]

        # Fallback: create a single segment using the filename as text.
        guessed_text = audio_path.stem.replace("_", " ")
        return [
            Segment(start=0.0, end=5.0, text=guessed_text, confidence=0.5, speaker=None)
        ]


class ASREngine:
    def __init__(
        self,
        backend: Optional[DummyWhisperBackend] = None,
        vad: Optional[VADSegmenter] = None,
        diarizer: Optional[Diarizer] = None,
        lang_identifier: Optional[LanguageIdentifier] = None,
        dialect_mapper: Optional[DialectMapper] = None,
    ) -> None:
        self.backend = backend or DummyWhisperBackend()
        self.vad = vad or VADSegmenter()
        self.diarizer = diarizer or Diarizer()
        self.lang_identifier = lang_identifier or LanguageIdentifier()
        self.dialect_mapper = dialect_mapper or DialectMapper()

    def _build_prompt_bias(self, options: TranscriptionOptions) -> Optional[str]:
        lexicon = options.custom_lexicon or []
        if options.context_prompt:
            lexicon.append(options.context_prompt)
        if not lexicon:
            return None
        return "\n".join(lexicon)

    def transcribe(self, audio_path: Path, options: TranscriptionOptions) -> TranscriptionResult:
        audio_path = Path(audio_path)
        segments = self.backend.transcribe(audio_path, options, self._build_prompt_bias(options))

        diarized_segments = self._apply_diarization(segments, audio_path, options)
        processed_segments = [
            self._post_process_segment(segment, options) for segment in diarized_segments
        ]
        full_text = " ".join(segment.text.strip() for segment in processed_segments if segment.text).strip()

        dialect_text = None
        if options.enable_dialect_map:
            dialect_text = self.dialect_mapper.map_text(full_text)

        metadata = {
            "model": getattr(self.backend, "name", "unknown"),
            "timestamp": datetime.utcnow().isoformat(),
        }

        return TranscriptionResult(
            text=full_text,
            segments=processed_segments,
            metadata=metadata,
            dialect_mapped_text=dialect_text,
        )

    def _apply_diarization(
        self, segments: List[Segment], audio_path: Path, options: TranscriptionOptions
    ) -> List[Segment]:
        if not options.enable_diarization:
            return segments
        diarization_labels = self.diarizer.assign_speakers(audio_path, segments)
        for segment, speaker in zip(segments, diarization_labels):
            segment.speaker = speaker
        return segments

    def _post_process_segment(self, segment: Segment, options: TranscriptionOptions) -> Segment:
        text = segment.text
        language = segment.language or self.lang_identifier.detect(text, hint=options.language_hint)
        if options.enable_itn:
            text = inverse_text_normalize(text, language=language)
        text = normalize_text(text, language=language)
        if options.enable_punct:
            text = restore_punctuation(text, language=language)
        segment.text = text
        segment.language = language
        return segment


def load_engine() -> ASREngine:
    # Production deployments would inspect environment capabilities and instantiate
    # FasterWhisper-backed engines. For this reference implementation we expose the
    # abstraction so that unit tests can exercise the control flow without
    # heavyweight dependencies.
    return ASREngine()
